---
title: "20201023-p8133_computingassignment_jsg2145"
author: "Jared Garfinkel"
date: "10/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	cache = TRUE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Question 1

## Part a

```{r typeIerror}
typeIerr = function(n = 10000) {
  sim_dat = rbinom(n, 20, .25)
  output = NULL
  
  for (i in 1:n) {
    if (sim_dat[[i]] > 5) {
      output = rbind(output, sim_dat[[i]])
    }
  }
  
  typeIerror = length(output)/n
  return(typeIerror)
}

typeIerror = typeIerr()
```

The type I error rate under the null, response rate = 0.25, is `r typeIerror`.

```{r}
output = NULL
pow = function(n = 10000) {
  sim_dat = rbinom(n, 20, .4)
  
  for (i in 1:n) {
    if (sim_dat[[i]] > 5) {
      output = rbind(output, sim_dat[[i]])
    }
  }
  
  power = nrow(output)/n
  return(power)
}

power = pow()
```

The power under the alternative, response rate = 0.4, is `r power`.

```{r}
# The expected sample size is equal to 20 times the proportion of times the futility trial threshold is not reached plus 71 times the proportion of times the futility trial threshold is reached

exp_ss = 20*typeIerror + 71*(1-typeIerror)
```

The expected sample size under these simulations is `r exp_ss`, or `r ceiling(exp_ss)`.

## Part b

```{r}
gonogo = function(stage1, stage2, null, alt, n1, n2) {
  res = vector(mode = "list", length = stage1)
  for(i in 1:stage1) {
    for(j in 1:stage2){
      res[[c(i,j)]] = bind_cols("n1_response" = i, "n2_response" = j)
    }
    res[[i]] = map(res[[i]], ~bind_rows(.))
  }

  sum_df = bind_rows(res) %>% 
    mutate(errorI = dbinom(n1_response, stage1, null) * dbinom(n2_response, stage2, null),
           power = dbinom(n1_response, stage1, alt) * dbinom(n2_response, stage2, alt)) %>%
    filter(n1_response > n1-1,
           n1_response + n2_response > n2-1) %>% 
    summarize(sum_error = sum(errorI),
              sum_power = sum(power))
  
  return(sum_df)
}
```

### Expected number of drugs identified to be effective

```{r}
sum_df = gonogo(stage1 = 20, stage2 = 51, null = .25, alt = 0.4, n1 = 6, n2 = 24)
```

The type I error rate under the null is `r round(pull(sum_df, sum_error), 4)`.

The power under the alternative is `r round(pull(sum_df, sum_power), 4)`


The expected number of drugs given a go decision is the number of drugs that work times the power (the probability of rejecting the null under the alternative) plus the number of drugs that don't work times the type I error rate (the probability of rejecting the null when the null is true).

```{r}
exp_godrug = 5*pull(sum_df, sum_power) + 95*pull(sum_df, sum_error)
```

So the expected number of drugs given a go decision can be calculated to be `r round(exp_godrug, 4)` or `r ceiling(exp_godrug)`.

The expected number of patients enrolled to the trial is the number of expected patients in each trial times 100.

### Expected total number of patients enrolled to the 100 trials

The expected number of patients in each trial is the size of the futility trial times the type I error plus the size of the two-stage trial when the futility threshold is reached.

```{r}
exp_ssnull = 20*pbinom(5, 20, .25) + 71*(1-pbinom(5, 20, .25))
exp_ssalt = 20*pbinom(5, 20, .4) + 71*(1-pbinom(5, 20, .4))
exp_ss2 = 5/100*exp_ssalt + 95/100*exp_ssnull
```

The expected sample size of each trial is `r round(exp_ss2, 4)` or `r ceiling(exp_ss2)`.

So, the expected number of patients for all trials is about 4077.

### Expected number of patients not having an MRI response among the enrolled.


The number of patients expected to not have an MRI response among the enrolled will be the number of patients enrolled in each trial times the true response rate for the drug in that trial.

```{r}
norespratenull = 95 * .25 * exp_ssnull
norespratealt = 5 * .4 * exp_ssalt
norespratetot = norespratenull + norespratealt
```

The number of patients expected not to have a response is `r norespratetot`.

## Part c

### Expected number of drugs identified to be effective

```{r}
fixed = function(n, go, null, alt) {
  errorI = NULL
  power = NULL
  res = NULL
  for(i in 1:n) {
    errorI[[i]] = dbinom(i, n, null)
    power[[i]] = dbinom(i, n, alt)
    res[[i]] = bind_cols("i" = i, "errorI" = errorI[[i]], "power" = power[[i]])
  }

  fixed_df = res %>% 
    bind_rows() %>% 
    filter(i > go - 1) %>%
    summarize(sum_error = sum(errorI),
              sum_power = sum(power))
  
  return(fixed_df)
}
```

```{r}
fixed_df = fixed(n = 62, go = 22, null = 0.25, alt = 0.4)
fixed_df
```

```{r}
exp_eff = 5*pull(fixed_df, sum_power) + 95 * pull(fixed_df, sum_error)
```

The expected number of drugs to be considered effective will be `r round(exp_eff, 4)` or `r ceiling(exp_eff)`.

### Expected total number of patients enrolled to the 100 trials

The expected number of patients in the trial is 62 x 100 since there are always the same number of patients enrolled in each trial in a fixed design study.

There are 6200 patients enrolled in the fixed design study.

### Expected number of patients not having an MRI response among the enrolled.

```{r}
norespratenull2 = 95 * .25 * 62
norespratealt2 = 5 * .4 * 62
norespratetot2 = norespratenull2 + norespratealt2
```

The expected number of patients with no response is `r norespratetot2` in the fixed design study. Compare to `r norespratetot` in the two-stage design.

# Question 2

```{r}
output = vector(mode = "list", length = 20)
for (i in 1:20) {
  for (j in 1:51) {
    output[[c(i, j)]] = bind_cols("n1_response" = i, "n2_response" = j)
    output[[c(i, j)]]$x = list(seq(from = 0.01, to = 0.4, by = 0.003))
  }
  output[[i]] = map(output[[i]], ~bind_rows(.))
}


output = bind_rows(output) %>% 
  unnest(x) %>% 
  nest(x)

# output %>% 
#   unnest() %>% 
#   unnest(x)
```

```{r ev20}
prior = function(df = output, null = 0.25, stage1n = 20, stage2n = 71, n1 = 6, n2 = 24) {
  res = df %>% 
    unnest(data) %>% 
    mutate(pE1 = qbeta(null, n1_response, (stage1n-n1_response)),
           pE2 = qbeta(null, (n1_response+n2_response), (stage2n-n1_response-n2_response)),
           pS = qbeta(null, 25, 75))
  
  res  = res %>% 
    filter(pE1>pS + x,
           pE2>pS + x) %>% 
    # group_by(x) %>% 
    # mutate(alpha = 1 - length(x)/nrow(goutput)) %>% 
    # select(x, alpha) %>% 
    distinct()
  return(res)
}
```


```{r}
res = prior()
```

```{r}
res
```


According to my calculations, a delta of 0.151 and an alpha of approximately 0.154 will result in a go decision when the 1st stage trial has a response rate equal to or greater than 6.    

```{r}
set.seed(1)
n_sim = 10e5
sum(rbeta(n_sim, shape1 = 6.5, shape2 = 15.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + 0.151)) / n_sim
sum(rbeta(n_sim, shape1 = 5.5, shape2 = 16.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + 0.151)) / n_sim
```
A simulation confirms that when the response rate is 6, the alpha is greater than .154, and when the response rate is 5, the alpha is less than 0.154.

# Question 3

The priors can be updated at n = 21, n = 41, and n = 61. There should be a probability that the futility response of less than 6 out of 20 respond.

```{r}
n_sim = 10e5
futility = sum(rbeta(n_sim, .5, 1.5) > rbeta(n_sim, 25, 75) + 0.15)/n_sim
futility
```

```{r}
n_sim = 10e5
futility2 = sum(rbeta(n_sim, 6.5, 15.5) > rbeta(n_sim, 25, 75) + 0.15)/n_sim
futility2
```
```{r}
n_sim = 10e5
futility3 = sum(rbeta(n_sim, 12.5, 29.5) > rbeta(n_sim, 25, 75) + 0.15)/n_sim
futility3
```

```{r}
n_sim = 10e5
futility4 = sum(rbeta(n_sim, 18.5, 43.5) > rbeta(n_sim, 25, 75) + 0.15)/n_sim
futility4
```

```{r}
n_sim = 10e5
futility5 = sum(rbeta(n_sim, 24.5, 47.5) > rbeta(n_sim, 25, 75) + 0.15)/n_sim
futility5
```

When the null is true the type I error rate is x2 and when the alternative is true, the power is y2.

# Question 4

The stopping boundary should be set such that given a delta and alpha pair, the response rate at or above the boundary will result in a go decision and below the boundary will result in a no-go decision. The first stopping boundary is given as $S_{20} = 6$. The decision rule should be such that the delta and alpha can be applied to each interim analysis and achieve the final go decision of $S_{71} = 24$

Given:

pE ~ beta(0.5, 1.5)
pS ~ beta(25, 75)
$\delta$ = 0.15
$\alpha$ = 0.2

After the first 20 patients, pE ~ beta(0.5 + $n_1$, 1.5 + 20 - $n_1$), where $n_1$ is the number of responses in the first stage.

After 40 patients, pE ~ beta(0.5 + $n_1 + n_2$, 1.5 + 40 - $(n_1 + n_2)$).

pE is updated at 60 patients and 71 patients as such.

```{r}
df4_maker = function(n1 = 7) {
  output4 = vector(mode = "list", length = 20)
  for (i in 1:20) {
    for(j in 1:20) {
      output4[[c(i, j)]] = bind_cols("n1_response" = i, "n2_response" = j)
      output4[[c(i, j)]]$k = list(seq(from = 1, to = 20))
      output4[[c(i, j)]]$l = list(seq(from = 1, to = 11))
      output4[[c(i, j)]]$x = list(seq(from = .15, to = 0.35, by = 0.02))
      output4[[c(i, j)]]$n1 = n1
      # output4[[c(i, j)]]$n2 = list(seq(13, 14))
      # output4[[c(i, j)]]$n3 = list(seq(19, 21))
      output4[[c(i, j)]]$n4 = 24
    }
    output4[[i]] = map(output4[[i]], ~bind_rows(.))
  }
  df4 = bind_rows(output4) %>% 
    rename("n3_response" = k,
           "n4_response" = l) %>%
    unnest(x) %>% 
    unnest(n3_response) %>% 
    unnest(n4_response) %>% 
    nest(-x)
  return(df4)
}
```

By creating a dataframe of all possible outcomes it is possible to calculate the probability of each response rate for each interim analysis. This function creates the responses, there are 20 possible response rates in the first trial, 20 potential response rates in the 2nd trial, 20 potential response rates in the 3rd trial and 11 in the 4th and final trial. This means there are 20x20x20x11 potential outcomes, or 88,000.

```{r}
df5 = df4_maker()
```

```{r}
df5 = df5 %>% 
  mutate(n3 = list(19:21),
         n2 = list(13:14)) %>% 
  unnest(n2) %>% 
  unnest(n3)

# df4
```

I hypothesized that the cutoff rates for futility could be 6 or 7 in each interim analysis to reduce the number of potential futility response rates. With a futility response of 7 in the first trial, there can be a cutoff of 13 (7 + 6) or 14 (7 + 7) in the 2nd round. In the 3rd round there should be between 19 (7 + 6 + 6) and 21 ( 7 + 7 + 7) responses to avoid futility. There must be 24 responses ($S_{71} = 24$) to avoid futility in the final check.

```{r}
prior5 = function(df = df5, null = 0.25, stage1n = 20, stage2n = 40, stage3n = 60, stage4n = 71) {
  res5 = df %>% 
    mutate(pE1 = qbeta(null, (0.5+n1_response), (1.5+stage1n-n1_response)),
           pE2 = qbeta(null, (0.5+n1_response+n2_response), (1.5+stage2n-n1_response-n2_response)),
           pE3 = qbeta(null, (0.5+n1_response + n2_response + n3_response), (1.5+stage3n-n1_response-n2_response-n3_response)),
           pE4 = qbeta(null, (0.5+n1_response + n2_response + n3_response + n4_response), (1.5+stage4n-n1_response-n2_response-n3_response-n4_response)))
           # pS = dbeta(null+pull(df, x), 25, 75))
    # filter(n1_response > n1-1,
    #        (n1_response + n2_response) > n2-1,
    #        (n1_response + n2_response + n3_response) > n3-1,
    #        (n1_response + n2_response + n3_response + n4_response) > n4-1)
  return(res5)
}
```

I calculated the quantile of each response using an updated beta distribution for that number of responses in each trial. Then, I filtered on only the response rates that would pass the futility interim analysis.

When the null is true, there is a type I error rate of x1, and when the alternative is true, there is a power of y1.

```{r}
map_df5 = df5 %>% 
  mutate(data = map(data, prior5))
```

```{r}
map_df5_nested = map_df5 %>% 
  unnest() %>%
  mutate(pS = qbeta(.25, shape1 = 25, shape2 = 75)) %>% 
  group_by(x, n1_response, n2_response, n3_response, n4_response) %>% 
  filter(pE1 > pS + x,
         pE2 > pS + x,
         pE3 > pS + x,
         pE4 > pS + x) %>% 
  # ungroup() %>%
  # group_by(n1_response, n2_response, n3_response, n4_response) %>%
  # filter(n1_response > n1-1,
  #        (n1_response + n2_response) > n2-1,
  #        (n1_response + n2_response + n3_response) > n3-1,
  #        (n1_response + n2_response + n3_response + n4_response) > n4-1)
  nest()
```

```{r}
# map_df51_nested = map_df5_nested %>% 
#   mutate(data = map(data, ~filter(., alpha1 > 0.2,
#                                   alpha2 > 0.2,
#                                   alpha3 > 0.2,
#                                   alpha4 > 0.2)))
```

```{r}
map_df5_nested %>% 
  unnest() %>% 
  select(n1_response, n2_response, n3_response, n4_response, x, pE1, pE2, pE3, pE4, pS) %>% 
  distinct() %>% 
  filter(x == 0.15) %>% 
  head()
```

Then I added the posterior distribution, pS (given), to the dataframe.

The alpha values can be calculated to be the average of the values of pE that are greater than $p_S + \delta$ for all values of potential responses. For instance, if the response in the first trial is below the futility decision no more participants are recruited. Further, if the futility decision is reached in an earlier trial, then the stopping time is recorded at the end of that trial (n = 20, 40, 60 or 71).

So, only if the probability of observing a response rate for the prior based on the updated prior distribution (pE) is greater than the probability of choosing the same value under the posterior (pS) with a buffer of $\delta$ in less than $\alpha$ proportion of cases then the conditions are satisfied.

Using the direct calculation method, it is possible to determine that response rates in each of the 4 interim analyses of 10, 18, 26, and 30 will satisfy the conditions. So, we can use these as the futility cutoffs.

# alternative is true

```{r}
df4 = df4_maker()

# df4 = df4 %>% 
#   mutate(n2 = list(12:13),
#          n3 = list(18:20)) %>% 
#   unnest(n2) %>% 
#   unnest(n3)
```

```{r ev10}
prior4 = function(df= df4, null = 0.25, stage1n = 20, stage2n = 40, stage3n = 60, stage4n = 71) {
  # res = vector(mode = "list", length = 2)
  # for (i in 1:2) {
  res4 = df %>%
    group_by(n1_response, n2_response, n3_response, n4_response) %>% 
    mutate(pE1 = qbeta(null, (0.5+n1_response), (1.5+stage1n-n1_response)),
           pE2 = qbeta(null, (0.5+n1_response+n2_response), (1.5+stage2n-n1_response-n2_response)),
           pE3 = qbeta(null, (0.5+n1_response + n2_response + n3_response), (1.5+stage3n-n1_response-n2_response-n3_response)),
           pE4 = qbeta(null, (0.5+n1_response + n2_response + n3_response + n4_response), (1.5+stage4n-n1_response-n2_response-n3_response-n4_response))) 
           # pS = dbeta(null+pull(df, x), 25, 75))
    # filter(n1_response > n1-1,
    #        (n1_response + n2_response) > n2-1,
    #        (n1_response + n2_response + n3_response) > n3-1,
    #        (n1_response + n2_response + n3_response + n4_response) > n4-1)
  return(res4)
}
```

```{r}
map_df4 = df4 %>%
  mutate(data = map(data, ~prior4(df = ., null = 0.4, stage1n = 20, stage2n = 40, stage3n = 60, stage4n = 71)))
```

```{r}
dist4 = map_df4 %>%
  unnest() %>%
  mutate(pS = qbeta(.25, shape1 = 25, shape2 = 75)) %>%
  # filter((n1_response + n2_response) > n2-1,
  #        (n1_response + n2_response + n3_response) > n3-1) %>%
  group_by(x, n1_response, n2_response, n3_response, n4_response) %>%
  filter(pE1 > pS + x,
         pE2 > pS + x,
         pE3 > pS + x,
         pE4 > pS + x)
# group_by(n2, n3) %>%
# nest()
```

```{r}
dist4
```

As can be seen from the list of potential outcomes that satisfy the interim analyses, 

```{r}
# gg4 = dist4 %>% 
# #   group_by(x) %>% 
# # # filter(pE1>pS,
# # #        pE2>pS,
# # #        pE3>pS,
# # #        pE4>pS) %>%
#   # summarize(n = n(),
#   #           alpha1 = 1 - sum(pE1 > pS)/n,
#   #           alpha2 = 1 - sum(pE2 > pS)/n,
#   #           alpha3 = 1 - sum(pE3 > pS)/n,
#   #           alpha4 = 1 - sum(pE4 > pS)/n) %>%
#   # filter(alpha1 < 0.2,
#   #        alpha2 < 0.2,
#   #        alpha3 < 0.2,
#   #        alpha4 < 0.2) %>%
#   ggplot(aes(x = x)) +
#   geom_line(aes(y = alpha1), color = "red") + 
#   geom_line(aes(y = alpha2), color = "orange") +
#   geom_line(aes(y = alpha3), color = "yellow") +
#   geom_line(aes(y = alpha4), color = "green") +
#   facet_grid(n2~n3)
```

```{r}
# head(map_df4) %>% 
#   unnest()
# 
# dist4 %>% 
#   filter(alpha4 > alpha1)
# 
# rbeta(10, 6.5, 14.5)
# qbeta(.25, 5.5, 14.5)
# qbeta(.25, 25, 75)
# 
# gg4
```


```{r}
set.seed(719)
n_sim = 10e5
x = 0.15
dat = tibble(means = c(sum(rbeta(n_sim, shape1 = 10.5, shape2 = 11.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 9.5, shape2 = 12.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 18.5, shape2 = 23.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 17.5, shape2 = 24.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 26.5, shape2 = 35.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 25.5, shape2 = 36.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 30.5, shape2 = 41.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 29.5, shape2 = 42.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim))

dat
```

Under the null, an alpha of 0.61 with a delta of 0.15 can be used to mirror adaptive designs "go-no-go" described in lecture using the futility decision rules described above $(S_{20} = 10, S_{40} = 18, S_{60} = 26, S_{71} = 30).

```{r}
set.seed(719)
n_sim = 10e5
x = 0.15
dat = tibble(means = c(sum(rbeta(n_sim, shape1 = 9.5, shape2 = 12.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 8.5, shape2 = 13.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 16.5, shape2 = 25.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 15.5, shape2 = 26.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 24.5, shape2 = 37.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 23.5, shape2 = 38.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 28.5, shape2 = 43.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim,
                      sum(rbeta(n_sim, shape1 = 27.5, shape2 = 44.5) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x)) / n_sim))

dat
```


Under the alternative, an alpha of 0.45 and a delta of 0.15 can satisfy the adaptive design "go-no-go" described in lecture.

```{r}
res7 = vector(mode = "list", length = 72)
sum_alphas = vector(mode = "list", length = 72)
n_sim = 10e2
for (i in 1:72) {
  for (j in 1:71) {
    sum_alphas[[c(i, j)]] = sum(rbeta(n_sim, shape1 = .5 + i - 1, shape2 = 1.5 + j - i - 1) > rbeta(n_sim, shape1 = 25, shape2 = 75)) / n_sim 
    res7[[c(i, j)]] = bind_cols("alpha" = sum_alphas[[c(i, j)]], "i" = i, "j" = j)
  }
  res7[[i]] = map(res7[[i]], ~bind_rows(.))
}
```

```{r}
res7 = bind_rows(res7)
```

```{r}
res7 %>% 
  tibble() %>% 
  group_by(i, j) %>% 
  summarize(n = n(),
            alpha = alpha) %>% 
  filter(alpha >= 0.9) %>% 
  group_by(i, j) %>% 
  summarize(nj = n(), 
            nm = nj/n,
            alpha = alpha) %>% 
  arrange(desc(j), i)
```

The alpha required to achieve the beta prior based on the available data can be calculated.

A response rate of at least 25 is required to achieve the interim analysis $Pr(S_{71} \ge 24 | S_m = s)$.

```{r}
set.seed(1)
n_sim = 10e4
x = seq(0.1, 0.19, by = 0.01)
outputx = vector(mode = "list", length = 20)
res = vector(mode = "list", length = 20)
for (i in 1:20) {
  for (j in 1:10) {
    outputx[[c(i, j)]] = sum(rbeta(n_sim, shape1 = .5+i, shape2 =1.5+20-i) > (rbeta(n_sim, shape1 = 25, shape2 = 75) + x[[j]])) / n_sim
    res[[c(i, j)]] = bind_cols("alpha" = outputx[[c(i, j)]], "i" = i, "j" = j)
  }
  res[[i]] = map(res[[i]], ~bind_rows(.))
}
# outputx = bind_rows(outputx)
# outputx
res = bind_rows(res)
```

```{r}
res %>% 
  filter(alpha >= 0.9)
```

# Question 5

I explored the beta distribution using beta distribution functions in R. 

I simulated values of response rates and compared them to those response rates in a posterior distribution given by ~beta(25, 75).

After this assignment I learned a way to plan an interim analysis study design using Bayesian formulation. 

I updated the beta function for each potential outcome and compared it to the probability of the posterior distribution.

Burn in and interim analyses can improve the expected number of patients enrolled in the study for drugs that can be assumed to have no effect. 
